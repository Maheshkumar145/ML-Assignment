{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPK6tkqMlRvPokabfM37U7O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Maheshkumar145/ML-Assignment/blob/main/ML_Assignment_09.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. What is feature engineering, and how does it work? Explain the various aspects of feature engineering in depth?**\n",
        "\n",
        "**Ans:** Feature engineering involves several steps, including:\n",
        "\n",
        "**Data cleaning:** This involves removing or imputing missing values, correcting inconsistencies, and removing outliers.\n",
        "\n",
        "**Feature selection:** This involves selecting the most relevant features for the model. This can be done using statistical tests or by analyzing the correlation between features.\n",
        "\n",
        "**Feature extraction:** This involves creating new features from the existing data. This can be done using techniques such as principal component analysis (PCA) or clustering.\n",
        "\n",
        "**Feature transformation:** This involves transforming the data to make it more suitable for the model. This can be done using techniques such as normalization or logarithmic transformation."
      ],
      "metadata": {
        "id": "UBbG_w6LGUuM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. What is feature selection, and how does it work? What is the aim of it? What are the various methods of function selection?**\n",
        "\n",
        "**Ans:** Feature selection is a process in machine learning and data mining that involves selecting a subset of relevant features or variables to use in building a predictive model. The aim of feature selection is to reduce the dimensionality of the data and improve the performance of the model by reducing overfitting, increasing accuracy, and improving model interpretability.\n",
        "\n",
        "Feature selection works by evaluating each feature's contribution to the model and selecting the most informative and relevant features. There are various methods for feature selection, including filter methods, wrapper methods, and embedded methods."
      ],
      "metadata": {
        "id": "A0oaEXFLGTWx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Describe the function selection filter and wrapper approaches. State the pros and cons of each approach?**\n",
        "\n",
        "**Ans:** Feature selection is a crucial step in the machine learning process that involves selecting the most informative and relevant features or variables from a dataset. There are two primary approaches to feature selection: filter and wrapper methods.\n",
        "\n",
        "Filter methods involve evaluating each feature independently of the model and selecting the most relevant features based on some criteria, such as their correlation with the target variable or their mutual information with the target variable. These methods are fast, computationally efficient, and easy to implement, but they may not always result in the best subset of features for the model. Filter methods do not consider the interaction between features, and they may select redundant or irrelevant features, which can negatively impact the model's performance."
      ],
      "metadata": {
        "id": "Rulrtgh1GRqa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.**\n",
        "**i. Describe the overall feature selection process.**\n",
        "\n",
        "**Ans:** Feature selection is a crucial step in machine learning, which involves selecting the most relevant features from the dataset to train a model. Here's an overall feature selection process:\n",
        "\n",
        "Define the Problem\n",
        "\n",
        "Data Pre-processing\n",
        "\n",
        "Feature Extraction\n",
        "\n",
        "Feature Selection Techniques\n",
        "\n",
        "Feature Ranking\n",
        "\n",
        "Feature Subset Selection\n",
        "\n",
        "Model Training and Evaluation\n",
        "\n",
        "**4.ii. Explain the key underlying principle of feature extraction using an example. What are the most widely used function extraction algorithms?**\n",
        "\n",
        "**Ans:** There are various feature extraction algorithms used in machine learning, but some of the most widely used ones are:\n",
        "\n",
        "Principal Component Analysis (PCA): PCA is a linear transformation technique that reduces the dimensionality of the dataset by finding the principal components that explain the most variance in the data.\n",
        "\n",
        "Linear Discriminant Analysis (LDA): LDA is another linear transformation technique that maximizes the separation between classes by finding the features that discriminate the classes the most.\n",
        "\n",
        "Kernel PCA: Kernel PCA is a non-linear extension of PCA that uses kernel functions to transform the data into a higher dimensional space where the separation between classes is more pronounced.\n",
        "\n",
        "t-Distributed Stochastic Neighbor Embedding (t-SNE): t-SNE is a non-linear dimensionality reduction technique that preserves the local structure of the data and maps it to a low-dimensional space where it can be visualized.\n",
        "\n",
        "Convolutional Neural Networks (CNNs): CNNs are a type of neural network that automatically learns the relevant features from the input data by convolving filters over the input image."
      ],
      "metadata": {
        "id": "GSYeaak3GQFV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Describe the feature engineering process in the sense of a text categorization issue.**\n",
        "\n",
        "**Ans:**\n",
        "**Text Pre-processing:** Firstly, the text data needs to be pre-processed to remove any unwanted characters, punctuations, and stop words. You may also perform stemming or lemmatization to normalize the text and reduce the dimensionality of the feature space.\n",
        "\n",
        "**Feature Extraction:** In this step, you create a set of features that can represent the text documents. There are various feature extraction techniques such as bag-of-words, TF-IDF, word embeddings, and n-grams. The choice of technique depends on the nature of the text data and the problem at hand.\n",
        "\n",
        "**Feature Selection**: Once the features are extracted, you need to select the most relevant features that can help the model to classify the documents accurately. You can use techniques such as mutual information, chi-square test, or feature importance scores to select the features.\n",
        "\n",
        "**Feature Engineering:** In some cases, you may need to engineer new features from the existing features to improve the model's performance. For example, you can create features such as the length of the document, the number of unique words, or the presence of specific keywords that are relevant to the problem.\n",
        "\n",
        "**Model Training and Evaluation:** Finally, you can train a machine learning model using the selected and engineered features and evaluate its performance on a separate test set. If the performance is not satisfactory, you may need to repeat the feature engineering process with different techniques or parameters until you obtain a good model."
      ],
      "metadata": {
        "id": "0Al88bL8GNoi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. What makes cosine similarity a good metric for text categorization? A document-term matrix has two rows with values of (2, 3, 2, 0, 2, 3, 3, 0, 1) and (2, 1, 0, 0, 3, 2, 1, 3, 1). Find the resemblance in cosine.**\n",
        "\n",
        "**Ans:**  Cosine similarity is a good metric for text categorization because it measures the similarity between two text documents based on the angle between their feature vectors in a high-dimensional space. The angle between two vectors is a measure of their similarity, where the smaller the angle, the more similar the vectors.\n",
        "\n",
        "\n",
        "Dot Product = 22 + 31 + 20 + 00 + 23 + 32 + 31 + 03 + 1*1 = 28\n",
        "\n",
        "Magnitude of row 1 = sqrt(2^2 + 3^2 + 2^2 + 0^2 + 2^2 + 3^2 + 3^2 + 0^2 + 1^2) = sqrt(34)\n",
        "\n",
        "Magnitude of row 2 = sqrt(2^2 + 1^2 + 0^2 + 0^2 + 3^2 + 2^2 + 1^2 + 3^2 + 1^2) = sqrt(22)\n",
        "\n",
        "Cosine Similarit = 28 / (sqrt(34) * sqrt(22)) = 0.849\n",
        "\n"
      ],
      "metadata": {
        "id": "t9bn9YnWGL8j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7.What is the formula for calculating Hamming distance? Between 10001011 and 11001111, calculate the Hamming gap.**\n",
        "\n",
        "**Ans:** The Hamming distance between \"10001011\" and \"11001111\" is 2, since there are two positions where the characters differ.\n",
        "\n"
      ],
      "metadata": {
        "id": "-AOeCZD9GJU9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. State what is meant by  \"high-dimensional data set\"? Could you offer a few real-life examples? What are the difficulties in using machine learning techniques on a data set with many dimensions? What can be done about it?**\n",
        "\n",
        "**Ans:** A high-dimensional dataset is a dataset that contains a large number of features or variables relative to the number of samples. High-dimensional datasets are often referred to as \"big data\" and can present unique challenges for analysis and modeling.\n",
        "\n",
        "Real-life examples of high-dimensional datasets include:\n",
        "\n",
        "Genomics data, where a genome can contain millions of genetic variants (features) and a study may include thousands of individuals (samples)\n",
        "Text data, where the features may be individual words or phrases and the number of documents can be in the millions or billions\n",
        "Image data, where each pixel can be considered a feature and an image may contain millions of pixels\n",
        "Sensor data, where each sensor can generate multiple measurements (features) and a system may have many sensors collecting data over time\n",
        "The difficulties of using machine learning techniques on high-dimensional datasets include:\n",
        "\n",
        "The curse of dimensionality: As the number of features increases, the number of possible configurations of those features increases exponentially, making it difficult to model accurately and efficiently.\n",
        "Overfitting: With high-dimensional datasets, it is easy to overfit the data, which means that the model will perform well on the training data but poorly on new data.\n",
        "Computational complexity: With large numbers of features, the computational requirements for training a model can become prohibitively high.\n",
        "To address these challenges, a number of techniques have been developed, including:\n",
        "\n",
        "Feature selection: Choosing a subset of the features that are most important for the problem at hand can reduce the number of dimensions and improve model performance.\n",
        "Dimensionality reduction: Techniques such as principal component analysis (PCA) and t-SNE can be used to transform the high-dimensional data into a lower-dimensional representation that retains most of the important information.\n",
        "Regularization: Techniques such as L1 or L2 regularization can help prevent overfitting by penalizing large weights in the model.\n",
        "Ensemble methods: Combining multiple models or subsets of the data can help reduce overfitting and improve generalization performance."
      ],
      "metadata": {
        "id": "8rfq2SDmGGor"
      }
    }
  ]
}