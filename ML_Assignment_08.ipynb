{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMc8iQq7wayT/XatyM43I8z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Maheshkumar145/ML-Assignment/blob/main/ML_Assignment_08.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. What exactly is a feature? Give an example to illustrate your point.**\n",
        "\n",
        "**Ans:** Feature is an individual measurable property or characteristic of a phenomenon. Choosing informative, discriminating and independent features is a crucial element of effective algorithms in pattern recognition, classification and regression."
      ],
      "metadata": {
        "id": "lv2U0ggy8rZa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. What are the various circumstances in which feature construction is required?**\n",
        "\n",
        "**Ans:**Feature construction, also known as feature engineering, is the process of transforming raw data into a set of features that can be used as input for machine learning models. It is required in many machine learning applications to improve the accuracy and effectiveness of models. Here are some common circumstances in which feature construction is required:\n",
        "\n",
        "Data Sparsity\n",
        "\n",
        "Non-Linearity\n",
        "\n",
        "Domain Knowledge\n",
        "\n",
        "Data Augmentation\n",
        "\n",
        "Data Compression"
      ],
      "metadata": {
        "id": "KvTOjHhv8qTB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Describe how nominal variables are encoded?**\n",
        "\n",
        "**Ans:** Nominal variables are categorical variables with no inherent order or numerical value. In machine learning models, nominal variables need to be encoded as numerical values to be used as input. \n",
        "\n",
        "There are several ways to encode nominal variables in Python, some common techniques are:\n",
        "\n",
        "1.One-Hot Encoding\n",
        "\n",
        "2.Label Encoding"
      ],
      "metadata": {
        "id": "t4v-usgj8ozS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Describe how numeric features are converted to categorical features?**\n",
        "\n",
        "**Ans:**Numeric features are variables that have numerical values and are often used as input in machine learning models. Sometimes it may be necessary to convert numeric features into categorical features to better capture patterns in the data. This can be done by discretizing the numeric values into a set of categories.\n",
        "\n"
      ],
      "metadata": {
        "id": "pgyaWaRo8nh6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Describe the feature selection wrapper approach. State the advantages and disadvantages of this approach?**\n",
        "\n",
        "**Ans:** The wrapper approach to feature selection involves selecting features based on the performance of the machine learning model trained on subsets of features.The wrapper approach can be performed using different search algorithms such as forward selection, backward elimination, and recursive feature elimination.\n",
        "\n",
        "_Advantages of the wrapper approach:_\n",
        "\n",
        "* It can handle non-linear relationships between features and the target variable.\n",
        "* It can be used with any machine learning algorithm, and it is not limited to any specific type of model.\n",
        "\n",
        "_Disadvantages of the wrapper approach:_\n",
        "* It may overfit the model to the training data, leading to poor generalization performance on the test data.\n",
        "* It may select redundant features, which can increase the complexity of the model and reduce its interpretability."
      ],
      "metadata": {
        "id": "jYMnqEH_8lhR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. When is a feature considered irrelevant? What can be said to quantify it?**\n",
        "\n",
        "**Ans:** A feature is considered irrelevant if it does not contribute useful information to the machine learning model for predicting the target variable. An irrelevant feature can add noise and complexity to the model, which can reduce its performance and increase the risk of overfitting."
      ],
      "metadata": {
        "id": "pJTFeyH-8kEC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. When is a function considered redundant? What criteria are used to identify features that could be redundant?**\n",
        "\n",
        "**Ans:** A function or feature is considered redundant if it does not provide any additional information to the machine learning model beyond what is already provided by other features. Redundant features can add noise and complexity to the model, leading to overfitting and reduced performance.\n",
        "\n",
        "There are various criteria that can be used to identify features that could be redundant. Some of these criteria include:\n",
        "\n",
        "High correlation with other features\n",
        "\n",
        "Low feature importance score\n",
        "\n",
        "Low variance\n",
        "\n",
        "Low mutual information\n",
        "\n",
        "Similar performance with or without the feature"
      ],
      "metadata": {
        "id": "20tY_WCc8h9Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. What are the various distance measurements used to determine feature similarity?**\n",
        "\n",
        "**Ans:** Distance measurements are used to determine the similarity or dissimilarity between features in machine learning. The choice of distance measure depends on the nature of the data and the problem at hand. \n",
        "\n",
        "Some of the commonly used distance measures include:\n",
        "\n",
        "* Euclidean distance\n",
        "\n",
        "* Manhattan distance\n",
        "\n",
        "* Cosine distance\n",
        "\n",
        "* Hamming distance\n",
        "\n",
        "* Mahalanobis distance"
      ],
      "metadata": {
        "id": "xhlGogpM7MY6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. State difference between Euclidean and Manhattan distances?**\n",
        "\n",
        "**Ans:**\n",
        "\n",
        "**Euclidean distance:** This is the most commonly used distance measure in machine learning. It calculates the distance between two points in n-dimensional space as the square root of the sum of the squared differences between the corresponding coordinates. The Euclidean distance is defined as:\n",
        "\n",
        "d(x, y) = sqrt(sum(xi - yi)^2)\n",
        "\n",
        "where x and y are two n-dimensional vectors, and xi and yi are the ith components of x and y, respectively.\n",
        "\n",
        "**Manhattan distance:** This distance measure, also known as the \"taxi-cab\" distance, calculates the distance between two points in n-dimensional space as the sum of the absolute differences between the corresponding coordinates. The Manhattan distance is defined as:\n",
        "\n",
        "d(x, y) = sum|(xi - yi)|\n",
        "\n",
        "where x and y are two n-dimensional vectors, and xi and yi are the ith components of x and y, respectively."
      ],
      "metadata": {
        "id": "pHONZiw87Joj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10. Distinguish between feature transformation and feature selection?**\n",
        "\n",
        "**Ans:** Feature transformation and feature selection are two techniques used in machine learning to improve the quality of input features. Feature transformation involves transforming the original features into a new set of features using mathematical or statistical techniques.\n",
        "\n",
        "The goal of feature selection is to reduce the dimensionality of the data by removing irrelevant, redundant, or noisy features, making it easier for machine learning algorithms to learn and make predictions. Feature selection techniques can be divided into three categories: filter methods, wrapper methods, and embedded methods."
      ],
      "metadata": {
        "id": "koba8HIu7Hla"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**11. Make brief notes on any two of the following:**\n",
        "\n",
        "**1.SVD (Standard Variable Diameter Diameter):** \n",
        "\n",
        "Standard variable diameter refers to the process of transforming data to have a common range or scale across variables. It involves standardizing the values of each variable so that they have the same mean and standard deviation.\n",
        "\n",
        "**2.Collection of features using a hybrid approach:**\n",
        "\n",
        "A hybrid approach to feature selection involves combining multiple techniques to collect a set of features that are most relevant to the prediction task. This approach can involve using both filter and wrapper methods, as well as incorporating domain knowledge and expert opinion to identify important features.\n",
        "\n",
        "**3.The width of the silhouette:** The silhouette width is a measure of how well an object belongs to its assigned cluster compared to other clusters. It is calculated by measuring the distance between an object and other objects in its cluster, and the distance between an object and objects in other clusters. The silhouette width ranges from -1 to 1.\n",
        "\n",
        "**4.Receiver operating characteristic curve:** A Receiver Operating Characteristic (ROC) curve is a graphical representation of the performance of a binary classification model. It plots the true positive rate (sensitivity) against the false positive rate (1-specificity) for different threshold values. The ROC curve is useful for comparing the performance of different models and selecting the optimal threshold value for the classification task. The area under the ROC curve (AUC) is a common metric used to quantify the overall performance of the model."
      ],
      "metadata": {
        "id": "mWLRMfjc7F93"
      }
    }
  ]
}